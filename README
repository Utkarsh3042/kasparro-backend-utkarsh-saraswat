# Crypto ETL Backend System üöÄ

A production-ready cryptocurrency data aggregation and ETL (Extract, Transform, Load) system built with **FastAPI**, **Docker**, **PostgreSQL**, and **multi-source data ingestion** (CSV, CoinGecko API, CoinPaprika API).

**Assignment Submission for Kasparov Backend & ETL Systems**  
**Date:** December 26, 2025  
**Version:** 2.0.0

## üåü Key Achievements

- ‚úÖ **3 Data Sources** - CSV files, CoinGecko API, CoinPaprika API
- ‚úÖ **Data Normalization** - Canonical ID strategy with intelligent deduplication
- ‚úÖ **Multi-stage Dockerfile** - Optimized production build
- ‚úÖ **PostgreSQL Integration** - Schema initialization and persistence
- ‚úÖ **Auto-Start ETL** - Automatic data ingestion on container launch
- ‚úÖ **Comprehensive API** - 9 REST endpoints with OpenAPI documentation
- ‚úÖ **Robust Error Handling** - Retry logic with exponential backoff
- ‚úÖ **Production Ready** - Docker Compose orchestration, health checks, logging

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     API Gateway Layer                        ‚îÇ
‚îÇ              (FastAPI with OpenAPI Docs)                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                   ‚îÇ                   ‚îÇ
        ‚ñº                   ‚ñº                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Ingestion  ‚îÇ   ‚îÇ Normalization‚îÇ   ‚îÇ   Monitoring ‚îÇ
‚îÇ     Layer    ‚îÇ   ‚îÇ & Dedup Layer‚îÇ   ‚îÇ    Layer     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   ‚îÇ   ‚îÇ   ‚îÇ               ‚îÇ                   ‚îÇ
   ‚îÇ   ‚îÇ   ‚îÇ               ‚ñº                   ‚ñº
   ‚îÇ   ‚îÇ   ‚îÇ        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ   ‚îÇ   ‚îÇ        ‚îÇ  Canonical   ‚îÇ    ‚îÇ  Logging ‚îÇ
   ‚îÇ   ‚îÇ   ‚îÇ        ‚îÇ  ID Mapping  ‚îÇ    ‚îÇ  & Health‚îÇ
   ‚îÇ   ‚îÇ   ‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   ‚îÇ   ‚îÇ   ‚îÇ
   ‚ñº   ‚ñº   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Data Storage Layer                       ‚îÇ
‚îÇ    PostgreSQL DB + In-Memory Cache (Singleton)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ
        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           External Data Sources                       ‚îÇ
‚îÇ   CSV Files  ‚îÇ  CoinGecko API  ‚îÇ  CoinPaprika API    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Architecture Highlights

- **Multi-Source Ingestion**: Fetches data from 3 sources (CSV, CoinGecko, CoinPaprika)
- **Data Normalization**: Assigns canonical IDs (e.g., BTC‚Üíbtc, ETH‚Üíeth) to unify symbols across sources
- **Intelligent Deduplication**: Priority-based deduplication (CoinGecko > CoinPaprika > CSV)
- **Auto-Start ETL**: Automatically ingests data on container startup (configurable)
- **PostgreSQL Persistence**: Schema with `cryptocurrencies` and `source_mappings` tables
- **Docker Multi-Stage Build**: Optimized production image with builder and runtime stages
- **Health Monitoring**: `/health` endpoint with service status and metrics

## üöÄ Features

### **P0 - Foundation Layer** ‚úÖ
- ‚úÖ **CSV Data Source**: Ingest cryptocurrency data from CSV files (required for Module 0.1)
- ‚úÖ **Multi-Source Ingestion**: CoinGecko API + CoinPaprika API + CSV files
- ‚úÖ **RESTful API**: 9 endpoints with automatic OpenAPI documentation at `/docs`
- ‚úÖ **Multi-Stage Dockerfile**: Optimized production build (required for Module 0.3)
- ‚úÖ **PostgreSQL Integration**: Database schema with initialization scripts
- ‚úÖ **Auto-Start ETL**: Configurable automatic data ingestion on startup (Module 0.4)
- ‚úÖ **Health Monitoring**: `/health` endpoint for service status
- ‚úÖ **Comprehensive Testing**: Unit and integration tests with pytest

### **P1 - Growth Layer** ‚úÖ
- ‚úÖ **Data Normalization**: Canonical ID strategy with symbol mapping (Module 2)
- ‚úÖ **Intelligent Deduplication**: Priority-based deduplication across sources
- ‚úÖ **Statistical Analysis**: Market cap stats, top gainers/losers (`/stats`)
- ‚úÖ **Advanced Filtering**: Filter by source, symbol, canonical ID
- ‚úÖ **Clean Architecture**: Separation of concerns (ingestion, normalization, storage, API)
- ‚úÖ **Comprehensive Error Handling**: Retry logic with exponential backoff

### **P2 - Differentiation Layer** ‚úÖ
- ‚úÖ **Rate Limiting & Backoff**: Exponential backoff for API failures (3 retries, 2s base delay)
- ‚úÖ **Observability**: Structured logging with timestamps and log levels
- ‚úÖ **Failure Recovery**: Graceful degradation when CoinGecko rate limits exceeded
- ‚úÖ **Configuration Management**: Environment-based settings with .env file
- ‚úÖ **Docker Compose Orchestration**: One-command multi-container setup
- ‚úÖ **Volume Persistence**: PostgreSQL data persistence and development hot-reload

## üìÅ Project Structure

```
backend-etl-system/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py                    # FastAPI application entry point
‚îÇ   ‚îú‚îÄ‚îÄ config.py                  # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ startup.py                 # Auto-ingest on container startup
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ routes.py              # API endpoint definitions (9 endpoints)
‚îÇ   ‚îú‚îÄ‚îÄ ingestion/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py                # Base ingestion class
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ coingecko.py           # CoinGecko API integration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ coinpaprika.py         # CoinPaprika API integration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ csv_source.py          # CSV file ingestion
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schemas.py             # Pydantic data models (with canonical_id)
‚îÇ   ‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ storage.py             # Data storage with normalization
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ logger.py              # Logging configuration
‚îÇ       ‚îú‚îÄ‚îÄ normalizer.py          # Data normalization & deduplication
‚îÇ       ‚îî‚îÄ‚îÄ retry.py               # Retry logic utilities
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ generate_sample_csv.py     # Generate sample cryptocurrency CSV
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ crypto_data.csv            # Sample CSV data (5 coins)
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ test_api.py                # API endpoint tests
‚îú‚îÄ‚îÄ docs/                          # Additional documentation
‚îú‚îÄ‚îÄ init.sql                       # PostgreSQL schema initialization
‚îú‚îÄ‚îÄ .env                           # Environment variables (not in git)
‚îú‚îÄ‚îÄ .env.example                   # Environment template
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ Dockerfile                     # Multi-stage container build
‚îú‚îÄ‚îÄ docker-compose.yml             # Multi-container orchestration
‚îú‚îÄ‚îÄ requirements.txt               # Python dependencies
‚îú‚îÄ‚îÄ pytest.ini                     # Test configuration
‚îú‚îÄ‚îÄ run.ps1                        # PowerShell management script
‚îú‚îÄ‚îÄ Makefile                       # Make commands for Unix systems
‚îú‚îÄ‚îÄ TESTING.md                     # Testing documentation
‚îú‚îÄ‚îÄ QUICKSTART.md                  # Quick reference guide
‚îî‚îÄ‚îÄ README.md                      # This file
```

## üõ†Ô∏è Technology Stack

- **Backend Framework**: FastAPI 0.104.1 (async web framework)
- **Database**: PostgreSQL 15-alpine (production persistence)
- **Data Processing**: Pandas 2.1.3 (CSV parsing and data manipulation)
- **HTTP Client**: Requests 2.31+ with retry logic
- **Validation**: Pydantic v2 (data models with canonical_id field)
- **Testing**: Pytest 7.4+ with coverage
- **Containerization**: Docker with multi-stage builds & Docker Compose
- **Python**: 3.10+ (type hints, async/await)
- **Environment Management**: python-dotenv with pydantic-settings

### Data Sources
- **CoinGecko API**: Free tier (rate limited to 10-50 requests/minute)
- **CoinPaprika API**: Requires API key from https://coinpaprika.com/api
- **CSV Files**: Pandas-based ingestion with sample generator script

## ‚ö° Quick Start

### **Prerequisites**
- Python 3.10+ installed
- Docker & Docker Compose installed (recommended for production deployment)
- CoinPaprika API key ([Get one free here](https://coinpaprika.com/api))

### **Option 1: Docker Deployment** (‚≠ê Recommended)

**Windows (PowerShell):**
```powershell
# 1. Configure environment
Copy-Item .env.example .env
# Edit .env and add your COINPAPRIKA_API_KEY

# 2. Generate sample CSV data
python scripts/generate_sample_csv.py

# 3. Start all services (backend + PostgreSQL)
docker-compose up --build -d

# 4. View logs
docker-compose logs -f backend

# 5. Check health
curl http://localhost:8000/health
```

**Linux/Mac:**
```bash
# 1. Configure environment
cp .env.example .env
# Edit .env and add your COINPAPRIKA_API_KEY

# 2. Generate sample CSV data
python scripts/generate_sample_csv.py

# 3. Start all services
docker-compose up --build -d

# 4. View logs
docker-compose logs -f backend

# 5. Check health
curl http://localhost:8000/health
```

### **Option 2: Local Development** (Without Docker)

**Windows (PowerShell):**
```powershell
# Create virtual environment
python -m venv venv
venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Generate sample CSV
python scripts/generate_sample_csv.py

# Run the application
uvicorn src.main:app --reload --host 0.0.0.0 --port 8000
```

**Linux/Mac:**
```bash
# Create and activate virtual environment
python -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Generate sample CSV
python scripts/generate_sample_csv.py

# Run the application
uvicorn src.main:app --reload --host 0.0.0.0 --port 8000
```

### **Access Points**
- **API Base URL**: http://localhost:8000
- **Interactive API Docs**: http://localhost:8000/docs ‚≠ê (Best for testing)
- **Alternative Docs**: http://localhost:8000/redoc
- **Health Check**: http://localhost:8000/health

### **Auto-Start ETL Configuration**

The system can automatically ingest data on startup. Configure in `.env`:

```env
# Enable/disable auto-ingestion
AUTO_INGEST_ON_STARTUP=true

# Choose which sources to ingest (comma-separated)
AUTO_INGEST_SOURCES=csv,coingecko,coinpaprika
```

**Note**: CoinGecko free tier has rate limits (10-50 requests/minute). If you see 429 errors, the system will gracefully fall back to CSV and CoinPaprika sources.

## üìñ API Endpoints

The system provides 9 RESTful API endpoints. See [TESTING.md](TESTING.md) for comprehensive testing examples.

### **Core Endpoints**

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/health` | Service health check with metrics |
| POST | `/api/v1/ingest/csv` | Ingest from CSV file |
| POST | `/api/v1/ingest/coingecko` | Ingest from CoinGecko API |
| POST | `/api/v1/ingest/coinpaprika` | Ingest from CoinPaprika API |
| POST | `/api/v1/ingest/all` | Ingest from all 3 sources |
| GET | `/api/v1/data` | Retrieve all data with filtering |
| GET | `/api/v1/stats` | Market statistics |
| GET | `/api/v1/coins/{canonical_id}` | Get coin by canonical ID |
| DELETE | `/api/v1/data` | Clear all data |

### **Quick Examples**

#### **1. CSV Ingestion**
```bash
# Ingest from data/crypto_data.csv
curl -X POST "http://localhost:8000/api/v1/ingest/csv"

# Response:
{
  "source": "csv",
  "count": 5,
  "status": "success",
  "coins_fetched": ["bitcoin", "ethereum", "cardano", "solana", "polkadot"]
}
```

#### **2. Multi-Source Ingestion with Normalization**
```bash
# Ingest from all 3 sources
curl -X POST "http://localhost:8000/api/v1/ingest/all?coingecko_limit=100&coinpaprika_limit=50"

# Response shows deduplication:
{
  "sources": {
    "csv": {"count": 5, "status": "success"},
    "coingecko": {"count": 100, "status": "success"},
    "coinpaprika": {"count": 50, "status": "success"}
  },
  "total_fetched": 155,
  "total_stored": 120,  # Duplicates removed
  "duplicates_removed": 35
}
```

#### **3. Retrieve Data with Filtering**
```bash
# Get all data (paginated)
curl "http://localhost:8000/api/v1/data?page=1&page_size=20"

# Filter by source
curl "http://localhost:8000/api/v1/data?source=coingecko"

# Filter by symbol
curl "http://localhost:8000/api/v1/data?symbol=BTC"

# Get by canonical ID
curl "http://localhost:8000/api/v1/coins/btc"
```

#### **4. Market Statistics**
```bash
curl "http://localhost:8000/api/v1/stats"

# Response:
{
  "total_coins": 120,
  "total_market_cap": 2451678934567.89,
  "average_price": 18234.56,
  "top_gainers": [
    {"symbol": "SOL", "price_change_24h": 15.67, "name": "Solana"}
  ],
  "top_losers": [
    {"symbol": "ADA", "price_change_24h": -8.34, "name": "Cardano"}
  ]
}
```

### **Interactive API Documentation**

Visit **http://localhost:8000/docs** for:
- Complete API schema
- "Try it out" functionality for each endpoint
- Request/response examples
- Authentication testing (if enabled)

## üîç Data Normalization Strategy

The system implements a sophisticated normalization and deduplication strategy to handle data from multiple sources.

### **Canonical ID Mapping**

Each cryptocurrency is assigned a **canonical ID** to unify different symbol representations across sources:

```python
CANONICAL_MAPPINGS = {
    'BTC': 'btc',     # Bitcoin
    'ETH': 'eth',     # Ethereum
    'ADA': 'ada',     # Cardano
    'SOL': 'sol',     # Solana
    'DOT': 'dot',     # Polkadot
    # ... and more
}
```

### **Symbol Aliases**

Different sources may use different symbols for the same coin:

```python
SYMBOL_ALIASES = {
    'WBTC': 'BTC',    # Wrapped Bitcoin ‚Üí Bitcoin
    'WETH': 'ETH',    # Wrapped Ethereum ‚Üí Ethereum
    'USDT': 'USDT',   # Tether remains USDT
}
```

### **Deduplication Logic**

When the same cryptocurrency appears in multiple sources, the system uses **source priority**:

**Priority Order**: CoinGecko (3) > CoinPaprika (2) > CSV (1)

**Example**: If Bitcoin appears in all 3 sources, only the CoinGecko version is kept.

```python
# Before normalization: 155 records fetched
# - CSV: 5 records (bitcoin, ethereum, cardano, solana, polkadot)
# - CoinGecko: 100 records (includes bitcoin, ethereum, etc.)
# - CoinPaprika: 50 records (includes bitcoin, ethereum, etc.)

# After deduplication: ~120 unique records
# - Bitcoin: 1 record (from CoinGecko, CSV and CoinPaprika versions removed)
# - Ethereum: 1 record (from CoinGecko, CSV and CoinPaprika versions removed)
# - New coins: Kept from their original source
```

### **Database Schema**

**`cryptocurrencies` table:**
```sql
CREATE TABLE cryptocurrencies (
    canonical_id VARCHAR(50) PRIMARY KEY,  -- Unique canonical ID (e.g., 'btc')
    symbol VARCHAR(20),                    -- Display symbol (e.g., 'BTC')
    name VARCHAR(255),                     -- Full name
    price DECIMAL(20, 8),                  -- Current price
    market_cap DECIMAL(30, 2),             -- Market capitalization
    volume_24h DECIMAL(30, 2),             -- 24h trading volume
    price_change_24h DECIMAL(10, 4),       -- 24h price change %
    source VARCHAR(50),                    -- Data source (winning source after dedup)
    last_updated TIMESTAMP                 -- Last update timestamp
);
```

**`source_mappings` table:**
```sql
CREATE TABLE source_mappings (
    id SERIAL PRIMARY KEY,
    canonical_id VARCHAR(50),              -- Links to cryptocurrencies.canonical_id
    source VARCHAR(50),                    -- Source name (csv, coingecko, coinpaprika)
    source_id VARCHAR(100),                -- Original ID from that source
    symbol VARCHAR(20)                     -- Original symbol from that source
);
```

This allows tracking which sources provided data for each canonical cryptocurrency.

## üß™ Testing

Comprehensive testing documentation is available in [TESTING.md](TESTING.md).

### **Quick Test Commands**

**Windows (PowerShell):**
```powershell
# Run all tests
pytest -v

# Run with coverage
pytest --cov=src --cov-report=term-missing

# Test specific endpoint
pytest tests/test_api.py::test_health_check -v

# Using the test script
.\test.ps1
```

**Linux/Mac:**
```bash
# Run all tests
pytest -v

# Run with coverage
pytest --cov=src --cov-report=html

# Docker environment
docker-compose run backend pytest -v
```

### **Test Coverage**

The test suite covers:
- ‚úÖ Health check endpoint
- ‚úÖ CSV ingestion with sample data
- ‚úÖ CoinGecko API integration (with rate limit handling)
- ‚úÖ CoinPaprika API integration
- ‚úÖ Multi-source ingestion with deduplication
- ‚úÖ Data retrieval with filtering (by source, symbol, canonical ID)
- ‚úÖ Market statistics calculation
- ‚úÖ Error handling and retry mechanisms

### **Manual Testing**

See [QUICKSTART.md](QUICKSTART.md) for step-by-step manual testing guide using:
- Interactive API docs at http://localhost:8000/docs
- PowerShell commands (Windows)
- curl commands (Linux/Mac)

## üîß Development Setup

### **Local Development (Without Docker)**

#### **Windows (PowerShell)**

1. **Create Virtual Environment**
```powershell
python -m venv venv
```

2. **Activate Virtual Environment**
```powershell
venv\Scripts\activate
# You should see (venv) in your terminal prompt
```

3. **Install Dependencies**
```powershell
pip install -r requirements.txt
pip install -e .  # Install in development mode
```

4. **Configure Environment**
```powershell
# Copy template
Copy-Item .env.example .env

# Edit .env file and add your API key
code .env  # Opens in VS Code
# Or: notepad .env
```

5. **Run Development Server**
```powershell
uvicorn src.main:app --reload --host 0.0.0.0 --port 8000
```

#### **Linux/Mac**

1. **Create and Activate Virtual Environment**
```bash
python -m venv venv
source venv/bin/activate
```

2. **Install Dependencies**
```bash
pip install -r requirements.txt
pip install -e .
```

3. **Set Environment Variables**
```bash
cp .env.example .env
# Edit .env and add your COINPAPRIKA_API_KEY
```

4. **Run Development Server**
```bash
uvicorn src.main:app --reload --host 0.0.0.0 --port 8000
```

### **Development Workflow**

1. **Start the server** (with hot reload)
2. **Open API docs** at http://localhost:8000/docs
3. **Make code changes** - server will auto-reload
4. **Run tests** after changes: `pytest -v`
5. **Check logs** in the terminal for debugging

## üö¢ Deployment

### **Production Deployment Options**

#### **Option 1: Render.com** (Recommended for quick deployment)

1. **Push code to GitHub**
```bash
git init
git add .
git commit -m "Initial commit: Crypto ETL Backend System"
git remote add origin https://github.com/yourusername/crypto-etl-backend.git
git push -u origin main
```

2. **Create Render Account**
   - Go to https://render.com
   - Sign up or log in with GitHub

3. **Deploy PostgreSQL Database**
   - Click "New +" ‚Üí "PostgreSQL"
   - Name: `crypto-etl-db`
   - Free tier is sufficient for testing
   - Copy the **Internal Database URL** after creation

4. **Deploy Web Service**
   - Click "New +" ‚Üí "Web Service"
   - Connect your GitHub repository
   - Name: `crypto-etl-backend`
   - Environment: `Docker`
   - Add environment variables:
     - `DATABASE_URL`: (paste Internal Database URL from step 3)
     - `COINPAPRIKA_API_KEY`: (your API key)
     - `AUTO_INGEST_ON_STARTUP`: `true`
     - `AUTO_INGEST_SOURCES`: `csv,coingecko,coinpaprika`

5. **Access Your Deployment**
   - URL: `https://crypto-etl-backend.onrender.com`
   - API Docs: `https://crypto-etl-backend.onrender.com/docs`

**Note**: Free tier may spin down after inactivity. First request may take 30-60 seconds.

#### **Option 2: Railway.app**

1. **Push code to GitHub** (same as Option 1)

2. **Deploy on Railway**
   - Go to https://railway.app
   - Click "Start a New Project"
   - Choose "Deploy from GitHub repo"
   - Select your repository

3. **Add PostgreSQL**
   - In your project, click "New" ‚Üí "Database" ‚Üí "Add PostgreSQL"
   - Railway auto-configures `DATABASE_URL`

4. **Configure Environment Variables**
   - Go to your service ‚Üí "Variables"
   - Add:
     - `COINPAPRIKA_API_KEY`
     - `AUTO_INGEST_ON_STARTUP=true`
     - `AUTO_INGEST_SOURCES=csv,coingecko,coinpaprika`

5. **Generate Domain**
   - Go to "Settings" ‚Üí "Networking"
   - Click "Generate Domain"

#### **Option 3: Docker Compose (Self-Hosted)**

Deploy on your own server:

```bash
# On your server
git clone https://github.com/yourusername/crypto-etl-backend.git
cd crypto-etl-backend

# Configure environment
cp .env.example .env
# Edit .env with your settings

# Start services
docker-compose up -d

# View logs
docker-compose logs -f backend
```

Access at `http://your-server-ip:8000`

## üîê Security & Configuration

### **Security Features**
- ‚úÖ API keys stored in environment variables (`.env` not in git)
- ‚úÖ No sensitive data in version control (`.gitignore` configured)
- ‚úÖ CORS middleware configured for production
- ‚úÖ Input validation with Pydantic models
- ‚úÖ Rate limiting on external API calls
- ‚úÖ Multi-stage Docker build (smaller attack surface)

### **Environment Variables**

| Variable | Required | Default | Description |
|----------|----------|---------|-------------|
| `DATABASE_URL` | No | In-memory | PostgreSQL connection string |
| `COINPAPRIKA_API_KEY` | Yes | - | API key from coinpaprika.com |
| `COINGECKO_API_URL` | No | `https://api.coingecko.com/api/v3` | CoinGecko base URL |
| `COINPAPRIKA_API_URL` | No | `https://api.coinpaprika.com/v1` | CoinPaprika base URL |
| `AUTO_INGEST_ON_STARTUP` | No | `false` | Auto-ingest data on container start |
| `AUTO_INGEST_SOURCES` | No | `csv,coingecko,coinpaprika` | Comma-separated source list |
| `LOG_LEVEL` | No | `INFO` | Logging level (DEBUG, INFO, WARNING, ERROR) |

### **Configuration Example**

```env
# Database (optional - uses in-memory if not provided)
DATABASE_URL=postgresql://crypto_user:crypto_pass@localhost:5432/crypto_etl

# Required: CoinPaprika API Key
COINPAPRIKA_API_KEY=your_api_key_here

# Optional: API URLs (defaults are fine)
COINGECKO_API_URL=https://api.coingecko.com/api/v3
COINPAPRIKA_API_URL=https://api.coinpaprika.com/v1

# Optional: Auto-ingestion
AUTO_INGEST_ON_STARTUP=true
AUTO_INGEST_SOURCES=csv,coingecko,coinpaprika

# Optional: Logging
LOG_LEVEL=INFO
```

## üìä Monitoring & Logging

### **Health Check**

Monitor service health and get system metrics:

```bash
curl http://localhost:8000/health
```

**Response:**
```json
{
  "status": "healthy",
  "message": "Crypto ETL Backend is running",
  "version": "2.0.0",
  "total_records": 120,
  "features": [
    "csv_ingestion",
    "multi_source_ingestion",
    "data_normalization",
    "deduplication",
    "postgresql_ready"
  ],
  "timestamp": "2024-12-26T10:30:00Z"
}
```

### **Structured Logging**

All operations are logged with:
- **Timestamp**: ISO 8601 format
- **Log Level**: DEBUG, INFO, WARNING, ERROR
- **Component**: Source module (CoinGecko, CSV, API, etc.)
- **Message**: Operation details
- **Error Details**: Full traceback on exceptions

**Example Logs:**
```
2024-12-26 10:30:15 - INFO - CSV: Successfully ingested 5 coins from data/crypto_data.csv
2024-12-26 10:30:20 - INFO - CoinGecko: Fetching up to 100 coins
2024-12-26 10:30:25 - WARNING - CoinGecko: Rate limit exceeded (429), retrying in 4s...
2024-12-26 10:30:30 - INFO - DataNormalizer: Normalized 155 records to 120 unique (35 duplicates removed)
2024-12-26 10:30:35 - INFO - Storage: Added 5 new records with normalization
```

### **Docker Logs**

View real-time logs:
```bash
# All services
docker-compose logs -f

# Backend only
docker-compose logs -f backend

# PostgreSQL only
docker-compose logs -f postgres

# Last 100 lines
docker-compose logs --tail=100 backend
```

## üõ°Ô∏è Error Handling & Retry Logic

The system implements comprehensive error handling:

### **Retry Mechanism**

**Configuration:**
- **Max Retries**: 3 attempts
- **Base Delay**: 2 seconds
- **Exponential Backoff**: 2x multiplier (2s ‚Üí 4s ‚Üí 8s)
- **Max Delay**: 60 seconds

**Example:**
```
Attempt 1: Failed (network error)
  ‚Üí Wait 2 seconds
Attempt 2: Failed (timeout)
  ‚Üí Wait 4 seconds
Attempt 3: Failed (rate limit)
  ‚Üí Wait 8 seconds
Attempt 4: Success or final failure
```

### **Error Categories**

| Error Type | Handling Strategy |
|------------|------------------|
| **Network Failures** | Automatic retry with exponential backoff |
| **API Rate Limits** (429) | Graceful backoff, continue with other sources |
| **Invalid Data** | Log validation error, skip record, continue processing |
| **Timeouts** | Configurable timeout (30s), retry on failure |
| **Partial Failures** | One source failure doesn't block others |
| **Database Errors** | Log error, fallback to in-memory storage |

### **Graceful Degradation**

**Multi-Source Resilience:**
```python
# If CoinGecko fails (rate limited)
POST /api/v1/ingest/all

# Response:
{
  "sources": {
    "csv": {"status": "success", "count": 5},
    "coingecko": {"status": "error", "message": "Rate limit exceeded"},
    "coinpaprika": {"status": "success", "count": 50}
  },
  "total_stored": 55,  # Still got 55 coins from CSV + CoinPaprika
  "status": "partial_success"
}
```

The system continues operating even if one or more sources fail.

## ‚ö†Ô∏è Known Limitations

### **CoinGecko API Rate Limiting**

**Issue**: CoinGecko free tier has strict rate limits (10-50 requests/minute). You may see:
```
WARNING - CoinGecko: Rate limit exceeded (429), retrying in 4s...
ERROR - CoinGecko: Failed to fetch data after 3 retries
```

**Why This Happens**: 
- Free tier allows ~50 requests/minute
- Fetching 100 coins may require multiple API calls
- Rate limits reset every minute

**Solution**: This is **expected behavior** and handled gracefully:
1. System automatically retries with exponential backoff
2. If CoinGecko continues failing, data from CSV and CoinPaprika still works
3. For testing, use smaller limits: `POST /api/v1/ingest/coingecko?limit=10`
4. Or rely on CSV and CoinPaprika sources: `AUTO_INGEST_SOURCES=csv,coinpaprika`

**Production Options**:
- Upgrade to CoinGecko Pro API ($129/month for higher limits)
- Cache CoinGecko data and refresh periodically
- Primary rely on CoinPaprika + CSV, use CoinGecko as supplementary source

### **In-Memory Storage by Default**

By default, data is stored in-memory. To enable PostgreSQL persistence:
1. Ensure `DATABASE_URL` is set in `.env`
2. Start PostgreSQL with `docker-compose up postgres -d`
3. Data persists across restarts

## üîÑ Future Enhancements

### **Planned Features**
- [ ] Redis caching layer for API responses
- [ ] WebSocket support for real-time price updates
- [ ] Incremental data ingestion (only fetch changed data)
- [ ] GraphQL API layer alongside REST
- [ ] Prometheus metrics export
- [ ] Kubernetes deployment manifests
- [ ] Historical data tracking (price history over time)
- [ ] Alert system for price changes
- [ ] Admin dashboard UI

### **Optional Improvements**
- [ ] Data aggregation for hourly/daily averages
- [ ] More data sources (Binance, Kraken APIs)
- [ ] Email notifications for significant market events
- [ ] Machine learning price predictions
- [ ] API authentication and user management

## üîç Troubleshooting

### **Common Issues**

#### **1. CoinGecko 429 Rate Limit Errors** ‚ö†Ô∏è (Expected)
```
WARNING - CoinGecko: Rate limit exceeded (429), retrying in 4s...
```
**This is normal!** CoinGecko free tier is rate limited.

**Solutions:**
- Use smaller limits: `?limit=10` instead of `?limit=100`
- Rely on CSV + CoinPaprika: Set `AUTO_INGEST_SOURCES=csv,coinpaprika`
- Wait 1 minute between requests for rate limits to reset
- The system handles this gracefully and continues with other sources

#### **2. Docker Container Not Starting**
```bash
# Check if containers are running
docker-compose ps

# View error logs
docker-compose logs backend
docker-compose logs postgres

# Rebuild from scratch
docker-compose down -v  # Remove volumes
docker-compose build --no-cache
docker-compose up -d
```

#### **3. PostgreSQL Connection Error**
```
ERROR - Database: Connection failed
```
**Solution:**
```bash
# Ensure PostgreSQL is running
docker-compose ps postgres

# Check DATABASE_URL in .env
# Should be: postgresql://crypto_user:crypto_pass@postgres:5432/crypto_etl

# Restart PostgreSQL
docker-compose restart postgres
```

#### **4. "No module named 'src'" Error**
```bash
# Verify you're in the project root
pwd  # Should show: .../backend-etl-system

# Install in development mode (local development only)
pip install -e .

# For Docker, rebuild:
docker-compose build backend
```

#### **5. Port 8000 Already in Use**
```powershell
# Windows: Find and kill process
netstat -ano | findstr :8000
taskkill /PID <PID> /F

# Or use different port
uvicorn src.main:app --reload --port 8001
```

#### **6. CSV File Not Found**
```
ERROR - CSV: File not found: data/crypto_data.csv
```
**Solution:**
```bash
# Generate sample CSV
python scripts/generate_sample_csv.py

# Verify file exists
ls data/crypto_data.csv
```

### **Debug Checklist**

‚úÖ **Before Testing:**
1. Environment variables configured in `.env`
2. Sample CSV generated: `python scripts/generate_sample_csv.py`
3. Docker containers running: `docker-compose ps`
4. Health check passes: `curl http://localhost:8000/health`

‚úÖ **If Tests Fail:**
1. Check logs: `docker-compose logs -f backend`
2. Verify API key is correct in `.env`
3. Ensure port 8000 is not in use
4. Try rebuilding: `docker-compose build --no-cache`

‚úÖ **For API Errors:**
1. Check interactive docs: http://localhost:8000/docs
2. Verify request format matches schema
3. Look for error details in response JSON
4. Check backend logs for full traceback

## üìÑ License & Acknowledgments

### **License**
This project is licensed under the MIT License - see the LICENSE file for details.

### **Author**
**Student Submission** - Backend & ETL Systems Assignment  
**Institution**: Kasparov  
**Submission Date**: December 26, 2025  
**Version**: 2.0.0

### **Acknowledgments**

- **CoinGecko API** - Free cryptocurrency market data ([coingecko.com](https://www.coingecko.com/))
- **CoinPaprika API** - Comprehensive crypto information ([coinpaprika.com](https://coinpaprika.com/))
- **FastAPI** - Modern, fast web framework for building APIs ([fastapi.tiangolo.com](https://fastapi.tiangolo.com/))
- **PostgreSQL** - World's most advanced open source database ([postgresql.org](https://www.postgresql.org/))
- **Docker** - Containerization platform ([docker.com](https://www.docker.com/))
- **Kasparov** - Assignment specification and educational guidance

---

## üìû Quick Reference

### **Essential URLs**
- **API Base**: http://localhost:8000
- **Interactive Docs**: http://localhost:8000/docs ‚≠ê
- **Health Check**: http://localhost:8000/health
- **Alternative Docs**: http://localhost:8000/redoc

### **Quick Commands**

**Windows (PowerShell):**
```powershell
# Start system
docker-compose up -d

# View logs
docker-compose logs -f backend

# Stop system
docker-compose down

# Full reset
docker-compose down -v && docker-compose build --no-cache && docker-compose up -d
```

**Linux/Mac:**
```bash
# Start system
docker-compose up -d

# View logs
docker-compose logs -f backend

# Stop system
docker-compose down

# Full reset
docker-compose down -v && docker-compose build --no-cache && docker-compose up -d
```

---

## ‚úÖ Assignment Completion Summary

### **All Critical Gates Passing:**
- ‚úÖ **Module 0.1**: CSV Ingestion implemented (CSVDataSource class)
- ‚úÖ **Module 0.2**: No hardcoded secrets (environment variables)
- ‚úÖ **Module 0.3**: Multi-stage Dockerfile (builder + runtime)
- ‚úÖ **Module 0.4**: Auto-start ETL on container launch
- ‚úÖ **Module 0.5**: Executable system (docker-compose one-command start)
- ‚úÖ **Module 1**: Production deployment ready (Render/Railway guides)
- ‚úÖ **Module 2**: Data normalization with canonical IDs and deduplication

### **Feature Implementation:**

**P0 (Foundation) - 100% Complete:**
- ‚úÖ 3 data sources (CSV, CoinGecko, CoinPaprika)
- ‚úÖ 9 RESTful API endpoints
- ‚úÖ Multi-stage Dockerfile
- ‚úÖ PostgreSQL database with schema
- ‚úÖ Docker Compose orchestration
- ‚úÖ Comprehensive test suite

**P1 (Growth) - 100% Complete:**
- ‚úÖ Data normalization with canonical IDs
- ‚úÖ Intelligent deduplication (source priority)
- ‚úÖ Market statistics endpoint
- ‚úÖ Advanced filtering (source, symbol, canonical ID)
- ‚úÖ Clean architecture with separation of concerns
- ‚úÖ Auto-start ETL configuration

**P2 (Differentiation) - 100% Complete:**
- ‚úÖ Exponential backoff retry (3 attempts, 2s base delay)
- ‚úÖ Structured logging with timestamps
- ‚úÖ Graceful degradation (multi-source resilience)
- ‚úÖ Environment-based configuration
- ‚úÖ Volume persistence
- ‚úÖ Health monitoring with metrics

### **System Metrics:**
- **Total Lines of Code**: ~2,500 lines (Python)
- **Test Coverage**: Core functionality covered
- **API Endpoints**: 9 endpoints
- **Data Sources**: 3 sources with normalization
- **Container Count**: 2 (backend + PostgreSQL)
- **Deployment Ready**: ‚úÖ Yes (Render, Railway, self-hosted)

---

**üéØ Ready for Submission**  
All requirements met. System is production-ready and fully documented.

For deployment and final submission, see:
- [TESTING.md](TESTING.md) - Complete testing guide
- [QUICKSTART.md](QUICKSTART.md) - Quick reference
- **Deployment** section above for Render/Railway deployment steps